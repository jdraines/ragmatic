project_name: hello_printer  # The name of your project; Currently not used anywhere in the code ¯\_(ツ)_/¯
root_path: ./src  # a relative path in this case implies you will run the `ragmatic` command within this same directory
components:
  storage:
    localpy:
      data_type: omni
      type: pydict
      config:
        dirpath: ./data  # a relative path in this case implies you will run the `ragmatic` command within this same directory
        overwrite: True
  llms:
    openai:
      type: openai
      config:
        api_keyenvvar: OPENAI_API_KEY  # Assumes you will store your OpenAI API key in an environment variable
  summarizers:
    pycode:
      type: python_code
      config:
        llm: openai
  encoders:
    plaintext:
      type: hugging_face
      config:
        model_name: "dunzhang/stella_en_1.5B_v5"  # A model for encoding text, found on hugging face hub
        tokenizer_config:
          return_tensors: pt
          max_length: 1024
          truncation: True
          padding: max_length
        save_model: False
  rag_agents:
    pycode:
      type: python_code
      config:
        llm: openai
        storage: localpy
        encoder: plaintext
        n_nearest: 20  # Number of nearest neighbors to retrieve
        prompt: ""  # A prompt to use when querying the LLM, will be the header of the message to the LLM.
        system_prompt: "" # A system prompt to use when querying the LLM
pipelines:  # declare the pipelines you want to run. These can by run by calling `ragmatic run <pipeline_name>`
  ingest-python-codebase:
    - action: summarize  # This action uses the pycode summarizer to summarize a code base found at the relative path ./src, and store it in the localpy storage
      config:
        root_path: ./src
        summarizer: pycode
        storage: localpy
    - action: encode
      config:
        encoder: plaintext
        source: 
          type: storage
          config: localpy
        storage: localpy
rag:
  rag_agent: python_code
  llm: openai
